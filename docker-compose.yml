services:
  ollama:
    image: ollama/ollama:latest
    container_name: ollama
    restart: unless-stopped
    ports:
      - "11434:11434"
    environment:
      - OLLAMA_HOST=0.0.0.0
      # 긴 컨텍스트 대비 메모리 압박 시, 필요에 따라 활성화
      # - OLLAMA_KV_CACHE_TYPE=q8_0
    volumes:
      - ollama_models:/root/.ollama
    entrypoint: ["/bin/sh"]  # 실행 파일만 지정
    command:                 # 인수를 리스트로 전달
      - "-lc"
      - |
        if [ -z "$OLLAMA_USE_GPU" ]; then
          if command -v nvidia-smi >/dev/null 2>&1; then
            OLLAMA_USE_GPU="auto"
            echo "[ollama] GPU detected via nvidia-smi -> OLLAMA_USE_GPU=auto"
          elif [ -e /dev/nvidia0 ]; then
            OLLAMA_USE_GPU="auto"
            echo "[ollama] GPU device node found -> OLLAMA_USE_GPU=auto"
          else
            OLLAMA_USE_GPU="never"
            echo "[ollama] No GPU detected -> OLLAMA_USE_GPU=never"
          fi
        else
          echo "[ollama] OLLAMA_USE_GPU preset to $OLLAMA_USE_GPU"
        fi
        export OLLAMA_USE_GPU
        exec /bin/ollama serve
    healthcheck:
      test: ["CMD-SHELL", "OLLAMA_HOST=http://127.0.0.1:11434 ollama list >/dev/null 2>&1"]
      interval: 10s
      timeout: 5s
      retries: 20

  # Ollama 서버가 뜬 뒤, 모델을 미리 받아놓는 1회성 Job
  model_init:
    image: ollama/ollama:latest
    container_name: model_init
    depends_on:
      ollama:
        condition: service_healthy
    entrypoint: ["/bin/sh"]  # 실행 파일만 지정
    environment:
      # 컨테이너 내부의 CLI가 ollama 서버에 붙도록 호스트 변경
      - OLLAMA_HOST=http://ollama:11434
    command:                 # 인수를 리스트로 전달
      - "-lc"
      - |
        if [ -z "$OLLAMA_USE_GPU" ]; then
          if command -v nvidia-smi >/dev/null 2>&1; then
            OLLAMA_USE_GPU="auto"
            echo "[model_init] GPU detected via nvidia-smi -> OLLAMA_USE_GPU=auto"
          elif [ -e /dev/nvidia0 ]; then
            OLLAMA_USE_GPU="auto"
            echo "[model_init] GPU device node found -> OLLAMA_USE_GPU=auto"
          else
            OLLAMA_USE_GPU="never"
            echo "[model_init] No GPU detected -> OLLAMA_USE_GPU=never"
          fi
        else
          echo "[model_init] OLLAMA_USE_GPU preset to $OLLAMA_USE_GPU"
        fi
        export OLLAMA_USE_GPU
        ollama pull sam860/exaone-4.0:1.2b || true
    restart: "no"

  # server_mcp:
  #   build:
  #     context: ./server_mcp
  #     dockerfile: Dockerfile
  #   container_name: server_mcp
  #   restart: unless-stopped
  #   ports:
  #     - "8001:8001"
  #   environment:
  #     # LangChain-Ollama가 붙을 주소 (server_mcp.py에서 base_url을 이 값으로 쓰고 싶다면 ENV 참조하도록 변경 가능)
  #     - OLLAMA_BASE=http://ollama:11434
  #   depends_on:
  #     ollama:
  #       condition: service_healthy
  #     model_init:
  #       condition: service_completed_successfully
  #   healthcheck:
  #     test: ["CMD", "bash", "-lc", "curl -sf http://127.0.0.1:8001/openapi.json >/dev/null || exit 1"]
  #     interval: 10s
  #     timeout: 5s
  #     retries: 20

  server_router:
    build:
      # context: ./server_router
      context: .
      dockerfile: Dockerfile
    container_name: server_router
    restart: unless-stopped
    ports:
      - "8000:8000"
    environment:
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME}
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}
      - MCP_SERVER_URL=${MCP_SERVER_URL}
    depends_on:
      ollama:
        condition: service_healthy
      model_init:
        condition: service_completed_successfully
    extra_hosts:
      - "host.docker.internal:host-gateway"
    healthcheck:
      test: ["CMD", "bash", "-lc", "curl -sf http://127.0.0.1:8000/openapi.json >/dev/null || exit 1"]
      interval: 10s
      timeout: 5s
      retries: 20

volumes:
  ollama_models:
