# 파일명: docker-compose.cpu.yml (CPU 전용 설정)
services: # ------------------- Docker 서비스 정의 시작 -------------------
  ollama: # Ollama 메인 서버
    image: ollama/ollama:latest # 사용할 Ollama 이미지
    container_name: ollama      # 컨테이너 이름 고정
    restart: unless-stopped     # 서버 비정상 종료 시 자동 재시작
    ports:
      - "11434:11434"           # 호스트 PC와 컨테이너 포트 연결
    environment:
      - OLLAMA_HOST=0.0.0.0     # 외부 IP에서의 API 접근 허용
    volumes:
      - ollama_models:/root/.ollama # 다운로드한 모델을 호스트에 영구 저장
    
    # --- CPU 모드 강제 설정 ---
    entrypoint: ["/bin/sh"] # 컨테이너 실행 시 사용할 셸 지정
    command: # entrypoint 셸에서 실행할 명령어
      - "-lc"
      - |
        export OLLAMA_USE_GPU="never" # Ollama가 GPU를 사용하지 않도록 설정
        echo "[ollama] Forcing CPU mode (OLLAMA_USE_GPU=never)" # 로그에 CPU 모드 명시
        exec /bin/ollama serve # Ollama 서버 프로세스 실행
    # --- 여기까지 ---

    healthcheck: # 서비스 정상 작동 여부 검사
      test: ["CMD-SHELL", "OLLAMA_HOST=http://127.0.0.1:11434 ollama list >/dev/null 2>&1"] # list 명령어로 응답 확인
      interval: 10s # 10초마다 상태 검사
      timeout: 5s   # 5초 내 응답 필요
      retries: 20   # 실패 시 20회 재시도

  model_init: # 모델 사전 다운로드용 1회성 서비스
    image: ollama/ollama:latest # Ollama CLI가 포함된 이미지 사용
    container_name: model_init  # 컨테이너 이름 고정
    depends_on: # 실행 순서 및 의존성 정의
      ollama:   # 'ollama' 서비스에 의존
        condition: service_healthy # 'ollama'가 healthy 상태가 되면 실행
    entrypoint: ["/bin/sh"] # 컨테이너 실행 시 사용할 셸 지정
    environment:
      - OLLAMA_HOST=http://ollama:11434 # 연결할 Ollama 서버 (내부 네트워크 주소)
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME} # 다운받을 LLM 모델명 (Ollma 서버 기준)

    # --- CPU 모드 강제 설정 ---
    command: # entrypoint 셸에서 실행할 명령어
      - "-lc"
      - |
        export OLLAMA_USE_GPU="never" # 모델 다운로드 시 GPU 사용 안 함
        echo "[model_init] Forcing CPU mode (OLLAMA_USE_GPU=never)" # 로그에 CPU 모드 명시
        echo "[model_init] Pulling model: ${OLLAMA_MODEL_NAME}" # 다운로드할 모델명 로그 출력
        ollama pull ${OLLAMA_MODEL_NAME} || true # 모델 다운로드 (실패 시에도 다음 단계 진행)
    # --- 여기까지 ---
              
    restart: "no" # 작업 완료 후 재시작 안 함

  server_router: # FastAPI 등 사용자 정의 API 서버
    build:       # Docker 이미지 빌드 설정
      context: . # 현재 디렉토리(.)를 빌드 컨텍스트로 사용
      dockerfile: Dockerfile # 사용할 Dockerfile 이름
    container_name: server_router # 컨테이너 이름 고정
    restart: unless-stopped  # 서버 비정상 종료 시 자동 재시작
    ports:
      - "8000:8000" # 호스트 8000 포트를 컨테이너 8000에 연결
    environment: # .env 파일로부터 주입될 환경변수
      - OLLAMA_MODEL_NAME=${OLLAMA_MODEL_NAME} # 사용할 모델 이름
      - OLLAMA_BASE_URL=${OLLAMA_BASE_URL}     # 연결할 Ollama 서버 주소
      - MCP_SERVER_URL=${MCP_SERVER_URL}       # (주석처리됨) MCP 서버 주소
    depends_on: # 실행 순서 및 의존성 정의
      ollama:   # 'ollama' 서비스에 의존
        condition: service_healthy # 'ollama'가 healthy 상태가 되면 실행
      model_init: # 'model_init' 서비스에 의존
        condition: service_completed_successfully # 'model_init'이 성공적으로 완료되면 실행
    extra_hosts: # /etc/hosts 파일에 추가할 항목
      - "host.docker.internal:host-gateway" # 컨테이너가 호스트 PC와 통신할 때 사용
    healthcheck: # 서비스 정상 작동 여부 검사
      test: ["CMD", "bash", "-lc", "curl -sf http://1.2.3.4:8000/openapi.json >/dev/null || exit 1"] # OpenAPI 문서로 응답 확인
      interval: 10s # 10초마다 상태 검사
      timeout: 5s   # 5초 내 응답 필요
      retries: 20   # 실패 시 20회 재시도

volumes: # ------------------- Docker 볼륨 정의 -------------------
  ollama_models: # 모델 데이터를 영구 저장할 볼륨 이름